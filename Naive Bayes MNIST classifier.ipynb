{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier on MNIST\n",
    "My previous implementation was like naive bayes, naive. I didnt understand much about distributions and tensor operations so no shock the code didnt work. As a noob, it was hard for me to understand high level implementations of others. So I will try to explain as much as I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip,os,pickle,math\n",
    "\n",
    "def get_data(file_path, file_name):\n",
    "    with gzip.open(os.path.join(file_path, file_name), 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "dir_ =\n",
    "filename = \n",
    "train_x, train_y, test_x, test_y = get_data(dir_,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bayes rule says </br>\n",
    "$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$\n",
    "which means that the probability that a particular sample belongs to a particular class Y equal what probability that sample has according to the class distribution times the probability of a class in the dataset.\n",
    "\n",
    "For MNIST, we can say $y = argmax_i(P(Y_i|X))$, where $i$ is the number of classes.\n",
    "\n",
    "Lets focus on each term of $P(Y_i|X)$\n",
    "1. $P(X|Y_i)$ : to get this term we formulate a distribution of each class using a multivariate gaussian distribution $\\mathcal{N}(\\mu,\\Sigma)$, where $\\mu$ is mean for each pixel and $\\Sigma$ is the covariance matrix. Each image has 784 pixels. So our distribution will be of 784 dimensions. Also we assume that *each pixel is independent to other* so our covariance matrix will be a diagonal matrix. In my previous implementation, I was using a univariate normal distribution for each pixel for each class. Using multivariate distribution is much easier. So, for multivariate gaussian $P(X|Y_i)= \\frac{1}{\\sqrt{ (2\\pi)^D |\\Sigma| }} exp\\left({ -\\frac{1}{2}(X – \\mu)^T \\Sigma^{-1} (X – \\mu) }\\right)$\n",
    "\n",
    " The expression look daunting but we dont have to worry about it. Pytorch will directly give us the probabilites as you will see soon.\n",
    " \n",
    "2. $P(Y)$: This is relatively easy to calculate. It is simply: </br>$\\frac{\\text{instances of that class}}{\\text{total length of train set}}$\n",
    "\n",
    "3. $P(X)$: The good thing is that we can ignore this term. Since we are calculating argmax and $P(X)$ will be same for all the classes. But if you wanted to calculate it nevertheless, then we can use multivariate gaussian again created using mean and standard deviation of entire train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "class_stats = {}\n",
    "class_means_std = {}\n",
    "for x,y in zip(train_x,train_y):\n",
    "    if y not in class_stats:\n",
    "        class_stats[y]=[]\n",
    "        class_means_std[y] ={}\n",
    "    class_stats[y].append(x)\n",
    "\n",
    "for i in class_stats:\n",
    "    class_instances = torch.Tensor(class_stats[i])\n",
    "    mean = class_instances.mean(dim=0)\n",
    "    var = class_instances.var(dim=0)\n",
    "    class_means_std[i][\"mean\"]= mean\n",
    "    class_means_std[i][\"var\"]= var\n",
    "    d = torch.eye(len(var))\n",
    "    class_means_std[i][\"cov\"] = d*(var[None,:]+1e-01)\n",
    "    class_means_std[i][\"dist\"] = MultivariateNormal(class_means_std[i][\"mean\"],class_means_std[i][\"cov\"])\n",
    "    class_means_std[i][\"prob\"] = float(len(class_instances)) / len(train_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation tricks:\n",
    "Now that we have the mean, variance and distribution for each digits we can finally calculate $P(X|Y)$ but these values can be very small. To avoid that we take log of $P(Y|X)$. Since we are finding argmax and log is monotically increasing this results no problem. So:\n",
    "\n",
    "$y = argmax_i(\\log{P(Y_i|X)}) = argmax_i(\\log{P(X|Y)} + \\log{P(Y)})$ since we chose to ignore $P(x)$\n",
    "\n",
    "Note: With continous distributions, we usually get $\\log{P(X|Y)}$ using log_prob from distributions. The log_prob is log of the probability distribution function and not the actual probability of that sample(actual probability of a sample from continous distribution is 0 beacuse of infinite samples in the distribution) so its value can be greater than zero. I mentioned because I was perplexed with +ve values for log_probs. It turns out that PDFs give relative likelihood not actual probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 78.32\n"
     ]
    }
   ],
   "source": [
    "accurate = 0\n",
    "for x,y in zip(test_x,test_y):\n",
    "    max = -math.inf\n",
    "    index = None\n",
    "    for i in class_means_std:\n",
    "        prob = class_means_std[i][\"dist\"].log_prob(torch.Tensor(x)) + math.log(class_means_std[i][\"prob\"])\n",
    "        if prob >max:\n",
    "            index = i\n",
    "            max = prob\n",
    "    if index==y:\n",
    "        accurate+=1\n",
    "\n",
    "print(\"Accuracy {}\".format(accurate*100/len(test_x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
