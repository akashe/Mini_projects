{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "The basic idea behind Principal Component Analysis(PCA), is to reduce the features of data in such a way that we still have the highest amount of information from the original features. Mathematically, it is eigendecomposition of covariance matrix of the data.\n",
    "\n",
    "When not to use PCA:\n",
    "\n",
    "1. If the variance of data features is very less.\n",
    "2. If the features have huge outliers in the data(affects variance)\n",
    "\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Why do we find eigen vectors of mean zeroed covariance matrix?\n",
    "\n",
    "   Because we are not changing the data, the covariance remains the same but we look at the data with new orthogonal dimensions(eigen vectors of symmetric matrices are real and othrogonal).We choose these dimensions using the eigen values(which show variance of data when projected) of the eigen vectors\n",
    "   \n",
    "2. How do the eigen values of these eigen vectors show variance of the projection of original matrix along these vectors?\n",
    "\n",
    "    Ans: https://math.stackexchange.com/questions/2147211/why-are-the-eigenvalues-of-a-covariance-matrix-equal-to-the-variance-of-its-eige\n",
    "\n",
    "Note: the below implementation uses the covariance method and involves calculating covariance matrix which is not cost effective for bigger data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:\n",
    "\n",
    "Lets say we have the input data as $X \\in R^{n\\times p}$, where $n$ are the total observations, $p$ is the number of original features. We want to reduce the dimension from $p$ to $l$ such that $l<p$ and get the final matrix $W \\in R^{n\\times l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Shit the data to zero mean\n",
    "\n",
    "We find column wise mean for each feature $\\mu \\in R^{p \\times 1}$. We get $B$ from $X$ such that $B = X - h\\mu^{T}$, where, $h \\in R^{n \\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling:\n",
    "Depending on your problem, you should choose to scale the values using the standard derivation or not. Why choose scaling, to have your data points be independent of the units of their measurement. When to avoid scaling, when the variation of a feature is itself important for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnwise_mean(x):\n",
    "    assert len(x.shape)==2\n",
    "    \n",
    "    return x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_mean(x, mean, scale = True):\n",
    "    if scale:\n",
    "        return x - mean    \n",
    "    else:\n",
    "        return (x - mean)/(x.std(dim=0)+1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Find Covariance matrix:\n",
    "We find the covariance matrix,$C \\in R^{p \\times p}$ of the mean shifted matrix $B$.\n",
    "\\begin{equation*} C = \\frac{1}{n-1}B*B\\end{equation*}\n",
    "where $*$ is a conjugate transpose about which I have no idea but when all the numbers in $B$ are real the operation is simply a transpose. So, for most of our cases,\n",
    "\\begin{equation*} C = \\frac{1}{n-1}B^{T}B\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_matrix(x):\n",
    "    n = x.shape[0]\n",
    "    assert n>=1\n",
    "    \n",
    "    return x.T.mm(x)/(n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Get eigen vectors and eigen values:\n",
    "\n",
    "We get corresponding eigen vectors $V$ and eigen values $D$ from $C$. Since we did $B^{T}B$, the resultant matrix $C$ was symmetric so the eigen values should be real and the eigen vectors orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_vectors_and_values(x):\n",
    "    e,v = torch.symeig(x, eigenvectors=True)\n",
    "    \n",
    "    # the above eigen values are in descending order and we prefer them in descending order\n",
    "    sorted_e, indices = torch.sort(e,dim=0,descending=True)\n",
    "    sorted_v = v.index_select(dim=0,index=indices)\n",
    "    \n",
    "    return sorted_e,sorted_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Select a subset of eigen vectors using a data ratio:\n",
    "\n",
    "Its time to reduce the dimensions, given a user defined ratio default 90%, we select filters that contain as much information as the ratio from the original data. The cumulative energy content g for the jth eigenvector is the sum of the energy content across all of the eigenvalues from 1 through j after sorting them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_features(eigen_values,ratio=0.9):\n",
    "    sums = []\n",
    "    a = 0\n",
    "    for i in eigen_values:\n",
    "        a += i.item()\n",
    "        if a <0 :\n",
    "            print(\"recieved a non-negative eigen value, setting it as 0\")\n",
    "            a = 0\n",
    "        sums.append(a)\n",
    "    \n",
    "    # shortlist number of features using ratio\n",
    "    g_p = sums[-1]\n",
    "    l = len(sums) - 1\n",
    "    while(True):\n",
    "        l -= 1\n",
    "        if (float(sums[l])/g_p)<ratio and l>=0:\n",
    "            break\n",
    "    l += 2 # because l here refers to index values so final features should be index values + 1 and another +1 because\n",
    "    # the last l value failed\n",
    "    \n",
    "    return l  \n",
    "\n",
    "def transform_data(x,eigen_vectors,features):\n",
    "    return x.mm(eigen_vectors[:features].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import gzip,os,pickle,math\n",
    "\n",
    "def get_data(file_path, file_name):\n",
    "    with gzip.open(os.path.join(file_path, file_name), 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "dir_ = \"/home/akashe/PycharmProjects/data/MNIST\"\n",
    "filename = \"mnist.pkl.gz\"\n",
    "train_x, train_y, test_x, test_y = get_data(dir_,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For now, we will work only with train_x\n",
    "train_x_ = torch.as_tensor(train_x)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column_wise mean\n",
    "mean = columnwise_mean(train_x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift to zero mean with scaling\n",
    "b = shift_mean(train_x_,mean,scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get covariance matrix of b\n",
    "c = get_covariance_matrix(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get eigen vectors and values for c in descending order\n",
    "e,v = eigen_vectors_and_values(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total features for which 80% of information is there\n",
    "ratio = 0.9\n",
    "l = get_number_of_features(e,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features for 90.0% of the information 235\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of features for {ratio*100}% of the information {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 235])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_desired_features = l\n",
    "transformed_data = transform_data(b,v,no_of_desired_features)\n",
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing results from sklearn\n",
    "\n",
    "Note: sklearn approaches don't scale the inputs after shifting to zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(train_x)\n",
    "pca = PCA(n_components=0.90)\n",
    "Xt = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_pca_features=len(Xt[0])\n",
    "sklearn_pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting 235 features from my implementation vs 235 from sklearn\n"
     ]
    }
   ],
   "source": [
    "print(f'Getting {l} features from my implementation vs {sklearn_pca_features} from sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "2. https://www.youtube.com/watch?v=fkf4IBRSeEc\n",
    "3. In detail lecture: https://www.youtube.com/watch?v=WW3ZJHPwvyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
