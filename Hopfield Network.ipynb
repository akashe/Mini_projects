{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reason for interest in Hopfield Network:\n",
    "a) Unsupervised nature of algorithm\n",
    "b) Can be used to save memories\n",
    "Starting with implementation of a simple Hopfield Network.\n",
    "    - states to be 1 and -1\n",
    "    - fucn for sequential update of network\n",
    "    \n",
    "Questions:\n",
    "    - what do weights represent here..are they updatable?\n",
    "    - how many iterations of sequential update rule I have to do? untill they no more energy change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopfield Network Alogrithm:\n",
    "1. Input = N binary patterns that you want to remember. Each bit can be [0,1] or [-1,1]\n",
    "2. Initialize weight matrix, using the above patterns:\n",
    "    \n",
    "    for each pattern in input:\n",
    "        create new matrix following hebbian rule:\n",
    "\\begin{align*}W_{ij}&= \\frac{1}{n}\\sum_{\\mu=1}^{n}(bit_{i}^{\\mu})(bit_{j}^{\\mu}) \\text{ }if\\text{ } i \\ne j \\\\\n",
    "&= 0 \\text{ }if\\text{ } i = j\n",
    "\\end{align*}\n",
    "\n",
    "where n is the total patterns and $(bit_{i}^{\\mu})$ is bit i of pattern $\\mu$\n",
    "3. Initialize a random pattern, $V$ which is similar to input binary patterns. The pattern can be totally different.\n",
    "4. Start iterating through the matrix. Iterate through the matrix in not completely random order. like be random in rows and not in columns.\n",
    "    \n",
    "    if $\\sum_{j}W_{ij}V_i>0$ then $V_i=+1$\n",
    "    else $V_i=0 \\text{ }or -1$, depends on your input type.\n",
    "5. Keep iterating untill the energy hasn't changed. In other words if none of bits are changed over one complete pass over the $V$\n",
    "\n",
    "Now, Hopfield networks will identify the closest pattern to that of random pattern in point 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1]\n",
      "[1, 0, 1, 0, 1, 1]\n",
      "Weights : \n",
      "tensor([[ 1., -1.,  0., -1.,  0.,  0.],\n",
      "        [-1.,  1.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  1.,  1.],\n",
      "        [-1.,  1.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  1.,  1.],\n",
      "        [ 0.,  0.,  1.,  0.,  1.,  1.]])\n",
      "Random pattern:\n",
      "[1, 1, 1, 0, 0, 0]\n",
      "New v [0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch # could use numpy also\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "# high low\n",
    "high = 1\n",
    "low = 0\n",
    "\n",
    "## Starting Patterns\n",
    "pattern_length = 6\n",
    "no_of_starting_patterns = 2\n",
    "\n",
    "# patterns = [[random.choice([high,low]) for i in range(pattern_length)] for j in range(no_of_starting_patterns)]\n",
    "patterns = [[0, 1, 1, 1, 1, 1],\n",
    "[1, 0, 1, 0, 1, 1]]\n",
    "for i in patterns:\n",
    "    print(i)\n",
    "\n",
    "# Intializing weight matrix\n",
    "weights = torch.zeros([pattern_length,pattern_length])\n",
    "\n",
    "for i in range(pattern_length):\n",
    "    for j in range(pattern_length):\n",
    "        if i<=j:\n",
    "#             weights[i][j] = reduce(lambda x,y: x+y,[patterns[k][i]*patterns[k][j] \\\n",
    "#                                                     for k in range(no_of_starting_patterns)])/no_of_starting_patterns\n",
    "            weights[i][j] = reduce(lambda x,y: x+y,[(2*patterns[k][i]-1)*(2*patterns[k][j]-1) \\\n",
    "                                                    for k in range(no_of_starting_patterns)])/no_of_starting_patterns\n",
    "        elif i==j:\n",
    "            weights[i][j] = 0\n",
    "        else:\n",
    "            weights[i][j] = weights[j][i]\n",
    "print(\"Weights : \\n{}\".format(weights))\n",
    "\n",
    "# Random pattern\n",
    "# v = [random.choice([high,low]) for i in range(pattern_length)]\n",
    "v = [1, 1, 1, 0, 0, 0]\n",
    "print(\"Random pattern:\")\n",
    "print(v)\n",
    "\n",
    "# Updating V to closest pattern:\n",
    "while(True):\n",
    "    a = list(range(pattern_length))\n",
    "    random.shuffle(a)\n",
    "    for i in a:\n",
    "        updated_bits = 0\n",
    "        V_temp = torch.sum(weights[i]*v[i])\n",
    "        if V_temp>0:\n",
    "            v[i] = high\n",
    "            updated_bits += 1\n",
    "        else:\n",
    "            v[i] = low\n",
    "    if updated_bits == 0:\n",
    "        print(\"New v {}\".format(v))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. When I started with a random matrix and used energy approach to toggle digits in a pattern, then I got the same final pattern but it wont happen when i used a larger number of digits\n",
    "\n",
    "    Intial reaction: OMG!!!! The final nodes come out same for the same weights. Is this why they can act as memories??\n",
    "\n",
    "2. When I implemented an another version of it. Intializing weight mztrix with 3 memory patterns, it didnt really identify the closest memory.\n",
    "Tomorrow: recheck my code\n",
    "1) combining energy method for node update\n",
    "\n",
    "From further experimentation:\n",
    "with multiple intial memories they dont really come close to the nearest one. or maybe I am implementing wrong?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
